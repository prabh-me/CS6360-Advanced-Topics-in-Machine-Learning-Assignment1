{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS6510 Assignment 1 Code Skeleton\n",
    "# Please use this outline to implement your decision tree. You can add any code around this.\n",
    "\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myname = \"Prabhjot Singh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.use_entropy = False\n",
    "\n",
    "    def learn_with_calculate_entropy_and_gini_index(self, training_set, title):\n",
    "\n",
    "        self.all_attributes = title.copy()\n",
    "        self.types_of_attributes = {k:'numeric' for k in self.all_attributes.keys()}\n",
    "\n",
    "\n",
    "        self.list_of_vertex = {}\n",
    "        self.data=[]\n",
    "        self.tree_prune = True\n",
    "        for k in range(10,20):\n",
    "\n",
    "            self.use_gini_index = False\n",
    "\n",
    "            self.n_tile = k\n",
    "\n",
    "            for N in xrange(0, 10):\n",
    "\n",
    "                training_data = [x for i, x in enumerate(training_set) if (i-N) % 10 != 0]\n",
    "                cross_validation_data = [x for i, x in enumerate(training_set) if (i-N) % 10 == 0]\n",
    "\n",
    "                self.list_of_vertex.clear()\n",
    "                del self.data[:]\n",
    "                self.data = training_data\n",
    "\n",
    "                self.train( training_data , title )\n",
    "\n",
    "                if self.tree_prune:\n",
    "                    self.prune(cross_validation_data)\n",
    "                    \n",
    "                accuracy = self.calculate_classific_accuracy(cross_validation_data)\n",
    "\n",
    "                print(\"training accuracy=%.4f\" % accuracy)\n",
    "\n",
    "                f = open(myname+\"result.txt\", \"w\")\n",
    "                f.write(\"accuracy: %.4f\" % accuracy)\n",
    "                f.close()\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    def classify(self, classif_data):\n",
    "        \n",
    "        vertex_id = 'root_vertex'\n",
    "        self.major_type_output = False\n",
    "        result = 0\n",
    "\n",
    "        while vertex_id:\n",
    "\n",
    "            vertex = self.list_of_vertex[vertex_id]\n",
    "\n",
    "            if vertex['type'] == 'bottom':\n",
    "                return vertex['large_value']\n",
    "\n",
    "            attrib = vertex['column_selected']\n",
    "            tval = classif_data[self.all_attributes[attrib]]\n",
    "            vertex_id=None\n",
    "            if self.types_of_attributes[attrib] == 'numeric':\n",
    "                prev=0.0\n",
    "                \n",
    "                dictionaries = {self.list_of_vertex[Key]['bin']:Key for Key in vertex['child_vertexs']}\n",
    "                all_the_keys = dictionaries.keys()\n",
    "                all_the_keys.sort()\n",
    "\n",
    "                for data_value in all_the_keys:\n",
    "                    new_key = dictionaries[data_value]\n",
    "                    if prev < tval <= data_value:\n",
    "                        vertex_id = new_key\n",
    "                        break\n",
    "                    prev=data_value\n",
    "\n",
    "        return result\n",
    "\n",
    "    def generate_n_dimensions(self, K, set_data_data_range,attribute_vertexs,result_attributes):\n",
    "\n",
    "        length_of_data = len(set_data_data_range)\n",
    "        dall_data_values={}\n",
    "        all_data_values=[]\n",
    "\n",
    "        for i in set_data_data_range:\n",
    "            g = self.data[i][self.all_attributes[attribute_vertexs]]\n",
    "\n",
    "            if g not in dall_data_values:\n",
    "                dall_data_values[g]=[i]\n",
    "            else:\n",
    "                dall_data_values[g].append(i)\n",
    "\n",
    "            all_data_values.append(g)\n",
    "        all_data_values.sort()\n",
    "\n",
    "        set_of_results = {}\n",
    "\n",
    "        if length_of_data <= 2:\n",
    "\n",
    "            data_all_data_values = dall_data_values.keys()\n",
    "            data_all_data_values.sort()\n",
    "\n",
    "            for all_values in data_all_data_values:\n",
    "\n",
    "                if all_values not in set_of_results:\n",
    "                    set_of_results[all_values] = {}\n",
    "                    set_of_results[all_values]['bin_records'] = []\n",
    "                    set_of_results[all_values]['bin_records_result_count'] = {}\n",
    "\n",
    "                set_of_results[all_values]['bin_records']=dall_data_values[all_values]\n",
    "\n",
    "\n",
    "        else:\n",
    "            if (length_of_data < 2*K):\n",
    "                new_key = length_of_data/2\n",
    "            else:\n",
    "                new_key = K\n",
    "\n",
    "            dividend = length_of_data / new_key\n",
    "            rem = length_of_data % new_key\n",
    "            previous_value = 0\n",
    "\n",
    "            for i in range(1,new_key+1):\n",
    "\n",
    "                ki = i*dividend-1\n",
    "                if (i <= rem):\n",
    "                    ki += i\n",
    "                else:\n",
    "                    ki += rem\n",
    "\n",
    "                if all_data_values[ki] not in set_of_results:\n",
    "                    set_of_results[all_data_values[ki]] = {}\n",
    "                    set_of_results[all_data_values[ki]]['bin_records'] = []\n",
    "                    set_of_results[all_data_values[ki]]['bin_records_result_count'] = {}\n",
    "\n",
    "                for all_values in all_data_values[previous_value:ki+1]:\n",
    "                    if all_values in dall_data_values:\n",
    "                        set_of_results[all_data_values[ki]]['bin_records'].extend(dall_data_values[all_values])\n",
    "\n",
    "                        for v in dall_data_values[all_values]:\n",
    "                            g = self.data[v][self.all_attributes[result_attributes]]\n",
    "                            if g in set_of_results[all_data_values[ki]]['bin_records_result_count']:\n",
    "                                set_of_results[all_data_values[ki]]['bin_records_result_count'][g] += 1\n",
    "                            else:\n",
    "                                set_of_results[all_data_values[ki]]['bin_records_result_count'][g] = 1\n",
    "\n",
    "                        del dall_data_values[all_values]\n",
    "                previous_value = ki+1\n",
    "\n",
    "        return set_of_results\n",
    "\n",
    "\n",
    "    def train ( self, training_data, title):\n",
    "\n",
    "        self.end_target = 'quality'\n",
    "        self.list_of_vertex['root_vertex']={ 'key':'root_vertex', 'parent':None, 'type' : 'root_vertex', 'all_attributes':title.copy(), 'set_data_data_range': set(range(len(training_data))) , 'child_vertexs': [], 'bin':None, 'parentAttrib':None, 'height':0, 'column_selected':None, 'parentKey':None }\n",
    "\n",
    "        list_stack = []\n",
    "        list_stack.append('root_vertex')\n",
    "        self.edge_entropy_gain = False\n",
    "        while list_stack:\n",
    "\n",
    "            vertex_id = list_stack.pop(0)\n",
    "\n",
    "            vertex = self.list_of_vertex[vertex_id]\n",
    "\n",
    "            lv = [self.data[i][self.all_attributes[self.end_target]] for i in vertex['set_data_data_range']]\n",
    "\n",
    "            if len(lv):\n",
    "                large_value = max(lv,key=lv.count)\n",
    "                vertex['large_value']=large_value\n",
    "\n",
    "            vertex['total']=len(vertex['set_data_data_range'])\n",
    "\n",
    "            errCount=0\n",
    "            for i in vertex['set_data_data_range']:\n",
    "                if self.data[i][self.all_attributes[self.end_target]] != large_value:\n",
    "                    errCount +=1\n",
    "\n",
    "            vertex['error_for_majority']=errCount\n",
    "\n",
    "\n",
    "            if ( len(vertex['all_attributes']) == 1):\n",
    "                vertex['type']='bottom'\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                entropy_value1 = calculate_entropy_and_gini_index(self,vertex['set_data_data_range'],self.end_target,self.end_target)\n",
    "\n",
    "                if ( entropy_value1 == 0 ):\n",
    "                    vertex['type']='bottom'\n",
    "                    continue\n",
    "                else:\n",
    "                    best_attribute_to_split = ''\n",
    "                    comparison_value=1\n",
    "                    for data_vertex in set(set(vertex['all_attributes'])-set([self.end_target])):\n",
    "\n",
    "                        entropy_value2 = calculate_entropy_and_gini_index(self,vertex['set_data_data_range'],self.end_target,data_vertex)\n",
    "\n",
    "                        if entropy_value2 < comparison_value:\n",
    "                            bept2 = entropy_value2\n",
    "                            best_attribute_to_split = data_vertex\n",
    "\n",
    "\n",
    "\n",
    "                    if vertex['type'] != 'root_vertex':\n",
    "                        if self.edge_entropy_gain:\n",
    "                            if comparison_value >= entropy_value1:\n",
    "                                vertex['type']='bottom'\n",
    "                                continue\n",
    "\n",
    "\n",
    "\n",
    "                    if ( self.types_of_attributes[best_attribute_to_split] == 'numeric' ):\n",
    "                        split_result = self.generate_n_dimensions(self.n_tile, vertex['set_data_data_range'],best_attribute_to_split,self.end_target)\n",
    "                        divided_list = set( split_result.keys() )\n",
    "                    else:\n",
    "                        divided_list = set( self.data[i][self.all_attributes[best_attribute_to_split]] for i in vertex['set_data_data_range'] )\n",
    "\n",
    "\n",
    "                    vertex['column_selected']=best_attribute_to_split\n",
    "                    td=vertex['child_vertexs']\n",
    "\n",
    "                    for s in divided_list:\n",
    "\n",
    "                        if ( self.types_of_attributes[best_attribute_to_split] == 'numeric' ):\n",
    "                            set_of_rec = set( split_result[s]['bin_records'] )\n",
    "                        else:\n",
    "                            set_of_rec = set( i for i in vertex['set_data_data_range'] if self.data[i][self.all_attributes[best_attribute_to_split]] == s )\n",
    "\n",
    "                        vall_attributes = vertex['all_attributes'].copy()\n",
    "                        vall_attributes.pop(best_attribute_to_split)\n",
    "\n",
    "                        childvertex_id = vertex_id+\"--\"+best_attribute_to_split+\"--\"+str(s)\n",
    "                        td.append(childvertex_id)\n",
    "\n",
    "                        childvertex = {'type': 'subTree', 'key': childvertex_id, 'all_attributes': vall_attributes,\n",
    "                                       'set_data_data_range': set_of_rec, 'bin': s, 'child_vertexs': [],\n",
    "                                       'parentAttrib': best_attribute_to_split, 'parentKey': vertex_id,\n",
    "                                       'height': vertex['height'] + 1, 'column_selected': None}\n",
    "\n",
    "                        self.list_of_vertex[childvertex_id]=childvertex\n",
    "                        list_stack.append(childvertex_id )\n",
    "\n",
    "\n",
    "    def prune(self,cross_validation_data):\n",
    "\n",
    "\n",
    "        top_accuracy = self.calculate_classific_accuracy(cross_validation_data)\n",
    "        vertex_list = copy.deepcopy(self.list_of_vertex)\n",
    "\n",
    "        self.prune_the_entire_set(cross_validation_data)\n",
    "\n",
    "        accuracy = self.calculate_classific_accuracy(cross_validation_data)\n",
    "        if accuracy > top_accuracy:\n",
    "            top_accuracy = accuracy\n",
    "            vertex_list.clear()\n",
    "            vertex_list = copy.deepcopy(self.list_of_vertex)\n",
    "\n",
    "        while len(self.list_of_vertex.keys()) > 1:\n",
    "            pruning_list_stack = {}\n",
    "\n",
    "            for vertex_id in self.list_of_vertex:\n",
    "                vertex = self.list_of_vertex[vertex_id]\n",
    "                if vertex['type'] == 'bottom':\n",
    "                    if vertex['height'] not in pruning_list_stack:\n",
    "                        pruning_list_stack[vertex['height']] = []\n",
    "                    pruning_list_stack[vertex['height']].append(vertex_id)\n",
    "\n",
    "\n",
    "            visited = {}\n",
    "            mincost_complexity_fact = float('inf')\n",
    "            maximum_height_reached = max(pruning_list_stack.keys())\n",
    "            prunevertex_id = None\n",
    "\n",
    "            for height in reversed(xrange(1,maximum_height_reached+1)):\n",
    "\n",
    "                while pruning_list_stack[height]:\n",
    "\n",
    "                    vertex = self.list_of_vertex[pruning_list_stack[height].pop(0)]\n",
    "\n",
    "                    if vertex['key'] in visited:\n",
    "                        continue\n",
    "\n",
    "                    visited[vertex['key']]=1\n",
    "\n",
    "                    if vertex['parentKey']:\n",
    "                        if vertex['height'] not in pruning_list_stack:\n",
    "                            pruning_list_stack[vertex['height']] = []\n",
    "                        pruning_list_stack[vertex['height']].append(vertex['parentKey'])\n",
    "\n",
    "\n",
    "                    if vertex['type'] == 'bottom':\n",
    "                        vertex['errors'] = vertex['error_for_majority']\n",
    "                        vertex['bottom'] = 1\n",
    "                        vertex['cost_complexity_fact'] = float('inf')\n",
    "\n",
    "                    elif vertex['type'] != 'bottom':\n",
    "\n",
    "                        class_error = 0.0\n",
    "                        bottom =  0.0\n",
    "\n",
    "                        for ck in vertex['child_vertexs']:\n",
    "                            class_vertex = self.list_of_vertex[ck]\n",
    "                            class_error +=  class_vertex['errors']\n",
    "                            bottom += float(class_vertex['bottom'])\n",
    "\n",
    "                        vertex['bottom'] = bottom\n",
    "                        vertex['errors'] = class_error\n",
    "                        if float(bottom) != 1:\n",
    "                            vertex['cost_complexity_fact'] =  ( float(vertex['error_for_majority']) - class_error ) / float(bottom-1.0)\n",
    "                        else:\n",
    "                            vertex['cost_complexity_fact'] = float('-inf')\n",
    "\n",
    "                        if vertex['cost_complexity_fact'] <= mincost_complexity_fact:\n",
    "                            mincost_complexity_fact = vertex['cost_complexity_fact']\n",
    "                            prunevertex_id = vertex['key']\n",
    "\n",
    "\n",
    "\n",
    "            if prunevertex_id:\n",
    "                self.prune_sub_list(prunevertex_id)\n",
    "\n",
    "\n",
    "            accuracy = self.calculate_classific_accuracy(cross_validation_data)\n",
    "            if accuracy > top_accuracy:\n",
    "                top_accuracy = accuracy\n",
    "                vertex_list.clear()\n",
    "                vertex_list = copy.deepcopy(self.list_of_vertex)\n",
    "\n",
    "        self.list_of_vertex.clear()\n",
    "        self.list_of_vertex = vertex_list\n",
    "\n",
    "\n",
    "    def prune_the_entire_set(self,cross_validation_data):\n",
    "\n",
    "\n",
    "        top_accuracy = float('-inf')\n",
    "        vertex_list = copy.deepcopy(self.list_of_vertex)\n",
    "        savedlist_of_vertex = copy.deepcopy(self.list_of_vertex)\n",
    "\n",
    "        for cost_complexity_fact in [0.4,0.6,0.8,1.0]:\n",
    "\n",
    "            self.list_of_vertex.clear()\n",
    "            self.list_of_vertex = copy.deepcopy(savedlist_of_vertex)\n",
    "\n",
    "            pruning_list_stack = {}\n",
    "\n",
    "            for vertex_id in self.list_of_vertex:\n",
    "                vertex = self.list_of_vertex[vertex_id]\n",
    "                if vertex['type'] == 'bottom':\n",
    "                    if vertex['height'] not in pruning_list_stack:\n",
    "                        pruning_list_stack[vertex['height']] = []\n",
    "                    pruning_list_stack[vertex['height']].append(vertex_id)\n",
    "\n",
    "            visited = {}\n",
    "\n",
    "            maximum_height_reached = max(pruning_list_stack.keys())\n",
    "\n",
    "            for height in reversed(xrange(1,maximum_height_reached+1)):\n",
    "\n",
    "                while pruning_list_stack[height]:\n",
    "\n",
    "                    vertex = self.list_of_vertex[pruning_list_stack[height].pop(0)]\n",
    "\n",
    "                    if vertex['key'] in visited:\n",
    "                        continue;\n",
    "\n",
    "                    visited[vertex['key']]=1\n",
    "\n",
    "                    if vertex['parentKey']:\n",
    "                        if vertex['height'] not in pruning_list_stack:\n",
    "                            pruning_list_stack[vertex['height']] = []\n",
    "                        pruning_list_stack[vertex['height']].append(vertex['parentKey'])\n",
    "\n",
    "\n",
    "                    if vertex['type'] == 'bottom':\n",
    "\n",
    "                        vertex['errors'] = vertex['error_for_majority']\n",
    "                        vertex['bottom'] = 1\n",
    "\n",
    "                    if vertex['type'] != 'bottom':\n",
    "\n",
    "                        class_error = 0\n",
    "                        bottom = 0\n",
    "\n",
    "                        for k in vertex['child_vertexs']:\n",
    "\n",
    "                            class_vertex = self.list_of_vertex[k]\n",
    "                            class_error +=  class_vertex['errors']\n",
    "                            bottom += float(class_vertex['bottom'])\n",
    "\n",
    "                        vertex['bottom'] = bottom\n",
    "                        if vertex['error_for_majority']  > ( class_error + cost_complexity_fact*(bottom-1) ):\n",
    "                            vertex['errors'] = class_error\n",
    "                        else:\n",
    "                            self.prune_sub_list(vertex['key'])\n",
    "\n",
    "            accuracy = self.calculate_classific_accuracy(cross_validation_data)\n",
    "            if accuracy > top_accuracy:\n",
    "                top_accuracy = accuracy\n",
    "                vertex_list.clear()\n",
    "                vertex_list = copy.deepcopy(self.list_of_vertex)\n",
    "                self.cost_complexity_fact = cost_complexity_fact\n",
    "\n",
    "        self.list_of_vertex.clear()\n",
    "        self.list_of_vertex = vertex_list\n",
    "\n",
    "\n",
    "    def prune_sub_list(self,prunevertex_id):\n",
    "\n",
    "        vertex = self.list_of_vertex[prunevertex_id]\n",
    "        vertex['errors']= vertex['error_for_majority']\n",
    "        vertex['type']='bottom'\n",
    "        vertex['column_selected']=None\n",
    "        vertex['bottom']=1\n",
    "        vertex['cost_complexity_fact']=float('inf')\n",
    "\n",
    "        prunelist_stack = vertex['child_vertexs']\n",
    "\n",
    "        vertex['child_vertexs']=[]\n",
    "\n",
    "        while prunelist_stack:\n",
    "\n",
    "            prune_vertex_id = prunelist_stack.pop(0)\n",
    "            prune_vertex = self.list_of_vertex[prune_vertex_id]\n",
    "\n",
    "            if prune_vertex['type'] != 'bottom':\n",
    "                for ck in prune_vertex['child_vertexs']:\n",
    "                    prunelist_stack.append(ck)\n",
    "\n",
    "            del self.list_of_vertex[prune_vertex['key']]\n",
    "\n",
    "    def calculate_classific_accuracy(self,cross_validation_data):\n",
    "\n",
    "        cross_valid_output = []\n",
    "        for instance in cross_validation_data:\n",
    "            result = self.classify( instance[:-1] )\n",
    "            cross_valid_output.append( result == instance[-1])\n",
    "\n",
    "        accuracy = float(cross_valid_output.count(True))/float(len(cross_valid_output))\n",
    "\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_and_gini_index(tree,set_data_data_range,parameter_result,decision_make_params):\n",
    "\n",
    "    decision_list = {}\n",
    "\n",
    "    r = tree.all_attributes[parameter_result]\n",
    "    d = tree.all_attributes[decision_make_params]\n",
    "\n",
    "    if ( tree.use_entropy and tree.types_of_attributes[decision_make_params] == 'numeric' ):\n",
    "\n",
    "        split_result = tree.generate_n_dimensions(tree.n_tile, set_data_data_range,decision_make_params,parameter_result)\n",
    "\n",
    "        divided_list = set( split_result.keys() )\n",
    "\n",
    "        for s in divided_list:\n",
    "            decision_list[s]={}\n",
    "            decision_list[s]['count']=len(split_result[s]['bin_records'])\n",
    "\n",
    "            for g in split_result[s]['bin_records_result_count'].keys():\n",
    "                decision_list[s][g]=split_result[s]['bin_records_result_count'][g]\n",
    "\n",
    "\n",
    "    else:\n",
    "        for i in set_data_data_range:\n",
    "\n",
    "            e = tree.data[i]\n",
    "\n",
    "            if e[d] in decision_list:\n",
    "                decision_list[e[d]]['count'] += 1\n",
    "            else:\n",
    "                decision_list[e[d]] = {}\n",
    "                decision_list[e[d]]['count'] = 1\n",
    "\n",
    "                if e[r] in decision_list[e[d]]:\n",
    "                    decision_list[e[d]][e[r]] += 1\n",
    "                else:\n",
    "                    decision_list[e[d]][e[r]] = 1\n",
    "\n",
    "    en=0.0\n",
    "\n",
    "    if not tree.use_gini_index:\n",
    "\n",
    "        for dc in decision_list:\n",
    "            dct = decision_list[dc]['count']\n",
    "            for rc in decision_list[dc]:\n",
    "                if rc != 'count':\n",
    "                    rct = decision_list[dc][rc]\n",
    "                    if parameter_result == decision_make_params:\n",
    "                        en += -(rct * (math.log(rct,2)-math.log(len(set_data_data_range),2)) )/len(set_data_data_range)\n",
    "                    else:\n",
    "                        en += -(dct*rct * (math.log(rct,2)-math.log(dct,2)) )/(dct*len(set_data_data_range))\n",
    "\n",
    "    else:\n",
    "\n",
    "        if ( parameter_result == decision_make_params ):\n",
    "\n",
    "            en = 1.0\n",
    "\n",
    "            for dc in decision_list:\n",
    "                dct = decision_list[dc]['count']\n",
    "                for rc in decision_list[dc]:\n",
    "                    if rc != 'count':\n",
    "                        rct = decision_list[dc][rc]\n",
    "                        en = en-math.pow(float(rct)/float(len(set_data_data_range)),2)\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            for dc in decision_list:\n",
    "                dct = decision_list[dc]['count']\n",
    "                en += float(dct)/float(tot)\n",
    "                for rc in decision_list[dc]:\n",
    "                    if rc != 'count':\n",
    "                        rct = decision_list[dc][rc]\n",
    "                        en = en-(float(dct)*math.pow(float(rct)/float(dct),2))/(tot)\n",
    "\n",
    "\n",
    "\n",
    "    return en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decision_tree():\n",
    "\n",
    "    with open(\"/Users/b0203050/Downloads/wine-dataset.csv\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        title = {e: i for i, e in enumerate(next(reader))}\n",
    "        data = [map(convert_string_to_float,line) for line in reader]\n",
    "\n",
    "\n",
    "    K = 10\n",
    "    training_set = [x for i, x in enumerate(data) if i % K != 9]\n",
    "    test_set = [x for i, x in enumerate(data) if i % K == 9]\n",
    "\n",
    "    tree = DecisionTree()\n",
    "    tree.learn_with_calculate_entropy_and_gini_index( training_set, title)\n",
    "\n",
    "\n",
    "    set_of_results = []\n",
    "    for instance in test_set:\n",
    "        result = tree.classify( instance[:-1] )\n",
    "        set_of_results.append( result == instance[-1])\n",
    "\n",
    "    accuracy = float(set_of_results.count(True))/float(len(set_of_results))\n",
    "    print \"accuracy: %.4f\" % accuracy\n",
    "\n",
    "    f = open(myname+\"result.txt\", \"w\")\n",
    "    f.write(\"accuracy: %.4f\" % accuracy)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_float(string):\n",
    "    try:\n",
    "        return float(string)\n",
    "    except ValueError:\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy=0.8413\n",
      "training accuracy=0.8345\n",
      "training accuracy=0.8299\n",
      "training accuracy=0.7914\n",
      "training accuracy=0.8186\n",
      "training accuracy=0.8073\n",
      "training accuracy=0.8254\n",
      "training accuracy=0.8073\n",
      "training accuracy=0.8027\n",
      "training accuracy=0.8318\n",
      "training accuracy=0.8254\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    run_decision_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
